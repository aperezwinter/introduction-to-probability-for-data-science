{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "(a) Construct a dataset with $N=20$ samples, following the model\n",
    "$$\n",
    "y_n = \\sum_{p=0}^{d-1} \\theta_p L_p(x_n) + e_n\\,\n",
    "$$\n",
    "where $\\theta_0=1$, $\\theta_1=0.5$, $\\theta_2=1.5$, $\\theta_4=1$, for $-1<x<1$. Here, $L_p(x)$ is the Legendre polynomial of the pth order. The $N=20$ samples are random uniformly sampled from the interval $[-1,1]$. The noise samples $e_n$ are i.i.d. Gaussian with variance $\\sigma^2=0.25^2$. Plot the dataset using Python command $\\textrm{scatter}$.\n",
    "\n",
    "(b) Run the regression using the same model where $d=5$, without any regularization. Plot the predicted curve and overlay with the training samples.\n",
    "\n",
    "(c) Repeat (b) by running the regression with $d=20$. Explain your observations.\n",
    "\n",
    "(d) Increase the number of training samples $N$ to $N=50$, $N=500$, $N=5000$, and repeat (c). Explain your observations.\n",
    "\n",
    "(e) Construct a testing dataset with $M=1000$ testing samples. For each of the regression models trained in (b)-(d), compute the testing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import legendre\n",
    "from scipy import stats\n",
    "from IPython.display import display, Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundTruth:\n",
    "    \"\"\"Ground truth function using Legendre polynomials\"\"\"\n",
    "    def __init__(self, theta):\n",
    "        self.d = len(theta)\n",
    "        self.theta = theta\n",
    "        self.L_vec = [legendre(i) for i in range(self.d)]\n",
    "\n",
    "    def f(self, x):\n",
    "        f_values = self.theta[0] * self.L_vec[0](x)\n",
    "        for i in range(1, self.d):\n",
    "            f_values += self.theta[i] * self.L_vec[i](x)\n",
    "        return f_values\n",
    "\n",
    "    def plot_samples_and_truth(self, x, y, figsize=(8, 5)):\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.scatter(x, y, color='blue', label='$y_n$', alpha=0.7)\n",
    "        x_curve = np.linspace(-1, 1, 200)\n",
    "        plt.plot(x_curve, self.f(x_curve), color='green', label='$f(x)$', linewidth=2)\n",
    "        plt.xlabel('$x$', fontsize=12)\n",
    "        plt.ylabel('$y$', fontsize=12)\n",
    "        plt.legend()\n",
    "        plt.title('Ground Truth and Samples')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class LegendreRegression:\n",
    "    \"\"\"Legendre polynomial regression\"\"\"\n",
    "    def __init__(self, d=5):\n",
    "        self.d = d\n",
    "        self.L_vec = [legendre(i) for i in range(d)]\n",
    "        self.theta = np.zeros(d)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        X = np.column_stack([self.L_vec[i](x) for i in range(self.d)])\n",
    "        self.theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_hat = self.theta[0] * self.L_vec[0](x)\n",
    "        for i in range(1, self.d):\n",
    "            y_hat += self.theta[i] * self.L_vec[i](x)\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "def generate_samples(f, N, mu=0, std=0.25):\n",
    "    \"\"\"Generate N samples with Gaussian noise\"\"\"\n",
    "    x = np.random.uniform(-1, 1, N)\n",
    "    y = f(x) + np.random.normal(mu, std, N)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_truth_and_model(gt, model, x, y, figsize=(8, 5)):\n",
    "    \"\"\"Plot ground truth, model prediction, and training data\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(x, y, color='blue', label='Training data', alpha=0.7, zorder=5)\n",
    "    x_curve = np.linspace(-1, 1, 200)\n",
    "    plt.plot(x_curve, gt.f(x_curve), color='green', label='True $f(x)$', linewidth=2)\n",
    "    plt.plot(x_curve, model.predict(x_curve), color='red', linestyle='--', \n",
    "             label=f'Fitted model (d={model.d})', linewidth=2)\n",
    "    plt.xlabel('$x$', fontsize=12)\n",
    "    plt.ylabel('$y$', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.title(f'Legendre Regression (d={model.d}, N={len(x)})', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Note: According to assignment: θ₀=1, θ₁=0.5, θ₂=1.5, θ₄=1\n",
    "# For d=5, this means: θ₀=1, θ₁=0.5, θ₂=1.5, θ₃=0, θ₄=1\n",
    "d = 5\n",
    "theta = np.array([1, 0.5, 1.5, 0, 1])\n",
    "mu = 0\n",
    "std = 0.25\n",
    "N = 20\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create ground truth and model\n",
    "gt_d5 = GroundTruth(theta)\n",
    "model_d5 = LegendreRegression(d)\n",
    "\n",
    "print(f\"True coefficients: {theta}\")\n",
    "print(f\"Noise std: σ = {std}\")\n",
    "print(f\"Noise variance: σ² = {std**2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a): Generate and plot dataset with N=20 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "x_N20, y_N20 = generate_samples(gt_d5.f, N=20, mu=mu, std=std)\n",
    "\n",
    "# Plot\n",
    "gt_d5.plot_samples_and_truth(x_N20, y_N20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b): Regression with d=5, N=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and plot\n",
    "model_d5.fit(x_N20, y_N20)\n",
    "plot_truth_and_model(gt_d5, model_d5, x_N20, y_N20)\n",
    "\n",
    "# Calculate training error\n",
    "y_train_pred = model_d5.predict(x_N20)\n",
    "train_error_d5_N20 = np.mean((y_N20 - y_train_pred) ** 2)\n",
    "print(f\"Training MSE (d=5, N=20): {train_error_d5_N20:.6f}\")\n",
    "print(f\"Theoretical training error σ²(1-d/N): {std**2 * (1 - d/N):.6f}\")\n",
    "print(f\"Learned coefficients θ: {model_d5.theta}\")\n",
    "print(f\"True coefficients: {theta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c): Regression with d=20, N=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d20_N20 = LegendreRegression(d=20)\n",
    "model_d20_N20.fit(x_N20, y_N20)\n",
    "plot_truth_and_model(gt_d5, model_d20_N20, x_N20, y_N20)\n",
    "\n",
    "# Calculate training error\n",
    "y_train_pred = model_d20_N20.predict(x_N20)\n",
    "train_error_d20_N20 = np.mean((y_N20 - y_train_pred) ** 2)\n",
    "print(f\"Training MSE (d=20, N=20): {train_error_d20_N20:.6f}\")\n",
    "print(f\"Theoretical training error σ²(1-d/N): {std**2 * (1 - 20/20):.6f} (should be ~0)\")\n",
    "print(f\"Norm of coefficients: {np.linalg.norm(model_d20_N20.theta):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations for part (c):**\n",
    "\n",
    "When $d=20$ and $N=20$, we have a **deterministic system** ($d = N$). This leads to:\n",
    "\n",
    "1. **Perfect fit on training data**: The model can exactly interpolate all training points, resulting in near-zero training error.\n",
    "2. **Overfitting**: The model \"memorizes\" the noise in the training data rather than learning the underlying pattern.\n",
    "3. **Poor generalization**: The predicted curve shows wild oscillations between training points, indicating high variance.\n",
    "4. **Large coefficients**: The norm of coefficients is typically very large, making the model sensitive to small changes in the data.\n",
    "\n",
    "This is a classic example of the **bias-variance tradeoff**: we've reduced bias to near zero but increased variance dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d): Regression with d=20 and varying N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datasets with different N\n",
    "x_N50, y_N50 = generate_samples(gt_d5.f, N=50, mu=mu, std=std)\n",
    "x_N500, y_N500 = generate_samples(gt_d5.f, N=500, mu=mu, std=std)\n",
    "x_N5000, y_N5000 = generate_samples(gt_d5.f, N=5000, mu=mu, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 50\n",
    "model_d20_N50 = LegendreRegression(d=20)\n",
    "model_d20_N50.fit(x_N50, y_N50)\n",
    "plot_truth_and_model(gt_d5, model_d20_N50, x_N50, y_N50)\n",
    "\n",
    "y_train_pred = model_d20_N50.predict(x_N50)\n",
    "train_error_d20_N50 = np.mean((y_N50 - y_train_pred) ** 2)\n",
    "print(f\"Training MSE (d=20, N=50): {train_error_d20_N50:.6f}\")\n",
    "print(f\"Theoretical training error σ²(1-d/N): {std**2 * (1 - 20/50):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 500\n",
    "model_d20_N500 = LegendreRegression(d=20)\n",
    "model_d20_N500.fit(x_N500, y_N500)\n",
    "plot_truth_and_model(gt_d5, model_d20_N500, x_N500, y_N500)\n",
    "\n",
    "y_train_pred = model_d20_N500.predict(x_N500)\n",
    "train_error_d20_N500 = np.mean((y_N500 - y_train_pred) ** 2)\n",
    "print(f\"Training MSE (d=20, N=500): {train_error_d20_N500:.6f}\")\n",
    "print(f\"Theoretical training error σ²(1-d/N): {std**2 * (1 - 20/500):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 5000\n",
    "model_d20_N5000 = LegendreRegression(d=20)\n",
    "model_d20_N5000.fit(x_N5000, y_N5000)\n",
    "plot_truth_and_model(gt_d5, model_d20_N5000, x_N5000, y_N5000)\n",
    "\n",
    "y_train_pred = model_d20_N5000.predict(x_N5000)\n",
    "train_error_d20_N5000 = np.mean((y_N5000 - y_train_pred) ** 2)\n",
    "print(f\"Training MSE (d=20, N=5000): {train_error_d20_N5000:.6f}\")\n",
    "print(f\"Theoretical training error σ²(1-d/N): {std**2 * (1 - 20/5000):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations for part (d):**\n",
    "\n",
    "As $N$ increases while keeping $d=20$ fixed:\n",
    "\n",
    "1. **Ratio $d/N$ decreases**: The system becomes increasingly overdetermined ($N \\gg d$).\n",
    "2. **Overfitting reduces**: With more data, the model has less opportunity to \"memorize\" noise.\n",
    "3. **Training error increases toward $\\sigma^2$**: Expected training error $\\approx \\sigma^2(1 - d/N) \\to \\sigma^2$ as $N \\to \\infty$.\n",
    "4. **Model approaches true function**: The learned coefficients become more accurate.\n",
    "5. **Coefficient norm decreases**: The model becomes more stable.\n",
    "\n",
    "| N | $d/N$ | Expected Training Error |\n",
    "|---|-------|------------------------|\n",
    "| 20 | 1.0 | 0 |\n",
    "| 50 | 0.4 | 0.0375 |\n",
    "| 500 | 0.04 | 0.06 |\n",
    "| 5000 | 0.004 | 0.0622 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (e): Testing Error with M=1000 test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test dataset\n",
    "M = 1000\n",
    "x_test, y_test = generate_samples(gt_d5.f, N=M, mu=mu, std=std)\n",
    "\n",
    "# Compute test error for all models\n",
    "models = [\n",
    "    (model_d5, \"d=5, N=20\"),\n",
    "    (model_d20_N20, \"d=20, N=20\"),\n",
    "    (model_d20_N50, \"d=20, N=50\"),\n",
    "    (model_d20_N500, \"d=20, N=500\"),\n",
    "    (model_d20_N5000, \"d=20, N=5000\"),\n",
    "]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Testing Errors (M=1000 test samples)\")\n",
    "print(\"=\" * 50)\n",
    "test_errors = []\n",
    "for model, label in models:\n",
    "    y_test_pred = model.predict(x_test)\n",
    "    test_error = np.mean((y_test - y_test_pred) ** 2)\n",
    "    test_errors.append(test_error)\n",
    "    print(f\"{label}: Test MSE = {test_error:.6f}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"Noise variance σ² = {std**2:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test errors\n",
    "plt.figure(figsize=(10, 5))\n",
    "labels = [label for _, label in models]\n",
    "colors = ['green', 'red', 'orange', 'blue', 'purple']\n",
    "bars = plt.bar(labels, test_errors, color=colors, alpha=0.7)\n",
    "plt.axhline(y=std**2, color='black', linestyle='--', label=f'$\\\\sigma^2$ = {std**2}')\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Test MSE', fontsize=12)\n",
    "plt.title('Test Error Comparison')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=15)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add values on top of bars\n",
    "for bar, error in zip(bars, test_errors):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{error:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations for part (e):**\n",
    "\n",
    "The test error reveals the **generalization gap**:\n",
    "\n",
    "1. **d=5, N=20** (correct model): Test error close to $\\sigma^2$ - good generalization.\n",
    "2. **d=20, N=20** (overfitting): Very high test error despite near-zero training error - severe overfitting.\n",
    "3. **d=20, N≥50**: Test error decreases as N increases - more data reduces overfitting.\n",
    "4. **d=20, N=5000**: Test error approaches that of the correct model (d=5).\n",
    "\n",
    "The **expected test error** can be expressed as:\n",
    "$$E_{\\text{test}} = \\sigma^2 + \\sigma^2 \\cdot \\frac{d}{N} = \\sigma^2 \\left(1 + \\frac{d}{N}\\right)$$\n",
    "\n",
    "This shows the variance component of the error, which decreases as $N/d$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Additional Analysis\n",
    "\n",
    "The following sections extend Exercise 1 to deepen our understanding of linear regression concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bias-Variance Tradeoff Analysis\n",
    "\n",
    "The test error can be decomposed as:\n",
    "$$E_{\\text{test}} = \\text{Bias}^2 + \\text{Variance} + \\sigma^2$$\n",
    "\n",
    "We'll empirically estimate these components by running multiple experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bias_variance(d, N, n_experiments=100, n_test=500):\n",
    "    \"\"\"Estimate bias and variance empirically\"\"\"\n",
    "    x_test = np.linspace(-1, 1, n_test)\n",
    "    y_true = gt_d5.f(x_test)\n",
    "    \n",
    "    predictions = np.zeros((n_experiments, n_test))\n",
    "    \n",
    "    for i in range(n_experiments):\n",
    "        x_train, y_train = generate_samples(gt_d5.f, N=N, mu=mu, std=std)\n",
    "        model = LegendreRegression(d=d)\n",
    "        model.fit(x_train, y_train)\n",
    "        predictions[i] = model.predict(x_test)\n",
    "    \n",
    "    # Mean prediction\n",
    "    mean_pred = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Bias^2 = E[(E[f_hat] - f_true)^2]\n",
    "    bias_squared = np.mean((mean_pred - y_true) ** 2)\n",
    "    \n",
    "    # Variance = E[(f_hat - E[f_hat])^2]\n",
    "    variance = np.mean(np.var(predictions, axis=0))\n",
    "    \n",
    "    return bias_squared, variance\n",
    "\n",
    "# Analyze for different d values\n",
    "d_values = [1, 3, 5, 10, 15, 20]\n",
    "N_fixed = 50\n",
    "bias_list, var_list = [], []\n",
    "\n",
    "for d_val in d_values:\n",
    "    b, v = compute_bias_variance(d_val, N_fixed, n_experiments=50)\n",
    "    bias_list.append(b)\n",
    "    var_list.append(v)\n",
    "    print(f\"d={d_val:2d}: Bias²={b:.5f}, Variance={v:.5f}, Total={b+v+std**2:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bias-variance tradeoff\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(d_values, bias_list, 'b-o', label='Bias²', linewidth=2, markersize=8)\n",
    "plt.plot(d_values, var_list, 'r-o', label='Variance', linewidth=2, markersize=8)\n",
    "plt.plot(d_values, [b+v for b, v in zip(bias_list, var_list)], 'g--o', \n",
    "         label='Bias² + Variance', linewidth=2, markersize=8)\n",
    "plt.axhline(y=std**2, color='gray', linestyle=':', label=f'$\\\\sigma^2$ = {std**2}')\n",
    "plt.axvline(x=5, color='orange', linestyle='--', alpha=0.7, label='True d=5')\n",
    "plt.xlabel('Model Complexity (d)', fontsize=12)\n",
    "plt.ylabel('Error Component', fontsize=12)\n",
    "plt.title('Bias-Variance Tradeoff (N=50)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Fold Cross-Validation\n",
    "\n",
    "Cross-validation provides a more robust estimate of test error when we can't afford a separate test set. We use K-Fold CV to select the optimal model complexity $d$.\n",
    "\n",
    "$$\\text{CV}_K = \\frac{1}{K} \\sum_{k=1}^{K} \\text{MSE}_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv(x, y, d, K=5):\n",
    "    \"\"\"Perform K-fold cross-validation\"\"\"\n",
    "    n = len(x)\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "    fold_size = n // K\n",
    "    \n",
    "    cv_errors = []\n",
    "    for k in range(K):\n",
    "        # Split data\n",
    "        val_idx = indices[k*fold_size : (k+1)*fold_size]\n",
    "        train_idx = np.concatenate([indices[:k*fold_size], indices[(k+1)*fold_size:]])\n",
    "        \n",
    "        x_train, y_train = x[train_idx], y[train_idx]\n",
    "        x_val, y_val = x[val_idx], y[val_idx]\n",
    "        \n",
    "        # Fit and evaluate\n",
    "        model = LegendreRegression(d=d)\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_val)\n",
    "        cv_errors.append(np.mean((y_val - y_pred) ** 2))\n",
    "    \n",
    "    return np.mean(cv_errors), np.std(cv_errors)\n",
    "\n",
    "# Use N=100 dataset for CV\n",
    "x_cv, y_cv = generate_samples(gt_d5.f, N=100, mu=mu, std=std)\n",
    "\n",
    "# CV for different d values\n",
    "d_range = range(1, 25)\n",
    "cv_means, cv_stds = [], []\n",
    "\n",
    "for d_val in d_range:\n",
    "    mean, std_cv = k_fold_cv(x_cv, y_cv, d_val, K=5)\n",
    "    cv_means.append(mean)\n",
    "    cv_stds.append(std_cv)\n",
    "\n",
    "optimal_d = d_range[np.argmin(cv_means)]\n",
    "print(f\"Optimal d from 5-fold CV: {optimal_d}\")\n",
    "print(f\"CV Error at optimal d: {cv_means[optimal_d-1]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CV curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(list(d_range), cv_means, yerr=cv_stds, fmt='b-o', capsize=3, \n",
    "             linewidth=2, markersize=6, label='5-Fold CV Error')\n",
    "plt.axvline(x=optimal_d, color='red', linestyle='--', label=f'Optimal d = {optimal_d}')\n",
    "plt.axvline(x=5, color='green', linestyle=':', label='True d = 5')\n",
    "plt.xlabel('Model Complexity (d)', fontsize=12)\n",
    "plt.ylabel('Cross-Validation MSE', fontsize=12)\n",
    "plt.title('Model Selection via K-Fold Cross-Validation', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ridge Regularization (L2 Penalty)\n",
    "\n",
    "Ridge regression adds an L2 penalty to control model complexity:\n",
    "$$\\hat{\\theta}_{\\text{ridge}} = \\arg\\min_\\theta \\|y - X\\theta\\|_2^2 + \\lambda \\|\\theta\\|_2^2$$\n",
    "\n",
    "The closed-form solution is:\n",
    "$$\\hat{\\theta}_{\\text{ridge}} = (X^T X + \\lambda I)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression:\n",
    "    \"\"\"Legendre polynomial regression with L2 regularization\"\"\"\n",
    "    def __init__(self, d=5, lam=0.1):\n",
    "        self.d = d\n",
    "        self.lam = lam\n",
    "        self.L_vec = [legendre(i) for i in range(d)]\n",
    "        self.theta = np.zeros(d)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        X = np.column_stack([self.L_vec[i](x) for i in range(self.d)])\n",
    "        self.theta = np.linalg.inv(X.T @ X + self.lam * np.eye(self.d)) @ X.T @ y\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_hat = self.theta[0] * self.L_vec[0](x)\n",
    "        for i in range(1, self.d):\n",
    "            y_hat += self.theta[i] * self.L_vec[i](x)\n",
    "        return y_hat\n",
    "\n",
    "# Test with d=20, N=20 (overfitting case)\n",
    "lambdas = np.logspace(-4, 2, 30)\n",
    "train_errors_ridge = []\n",
    "test_errors_ridge = []\n",
    "coef_norms = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    model = RidgeRegression(d=20, lam=lam)\n",
    "    model.fit(x_N20, y_N20)\n",
    "    \n",
    "    train_errors_ridge.append(np.mean((y_N20 - model.predict(x_N20)) ** 2))\n",
    "    test_errors_ridge.append(np.mean((y_test - model.predict(x_test)) ** 2))\n",
    "    coef_norms.append(np.linalg.norm(model.theta))\n",
    "\n",
    "optimal_lam = lambdas[np.argmin(test_errors_ridge)]\n",
    "print(f\"Optimal λ: {optimal_lam:.4f}\")\n",
    "print(f\"Test error at optimal λ: {min(test_errors_ridge):.5f}\")\n",
    "print(f\"Test error without regularization (d=20, N=20): {test_errors[1]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Ridge regression results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Error vs lambda\n",
    "axes[0].semilogx(lambdas, train_errors_ridge, 'b-', label='Train Error', linewidth=2)\n",
    "axes[0].semilogx(lambdas, test_errors_ridge, 'r-', label='Test Error', linewidth=2)\n",
    "axes[0].axvline(x=optimal_lam, color='green', linestyle='--', label=f'Optimal λ = {optimal_lam:.4f}')\n",
    "axes[0].axhline(y=std**2, color='gray', linestyle=':', label=f'$\\\\sigma^2$ = {std**2}')\n",
    "axes[0].set_xlabel('λ (regularization)', fontsize=12)\n",
    "axes[0].set_ylabel('MSE', fontsize=12)\n",
    "axes[0].set_title('Ridge Regression: Error vs Regularization', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Coefficient norm vs lambda\n",
    "axes[1].semilogx(lambdas, coef_norms, 'purple', linewidth=2)\n",
    "axes[1].axvline(x=optimal_lam, color='green', linestyle='--', label=f'Optimal λ')\n",
    "axes[1].set_xlabel('λ (regularization)', fontsize=12)\n",
    "axes[1].set_ylabel('||θ||₂', fontsize=12)\n",
    "axes[1].set_title('Coefficient Shrinkage', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Numerical Conditioning Analysis\n",
    "\n",
    "The condition number of $X^TX$ determines numerical stability:\n",
    "$$\\kappa(X^TX) = \\frac{\\sigma_{\\max}(X^TX)}{\\sigma_{\\min}(X^TX)}$$\n",
    "\n",
    "A high condition number indicates the problem is ill-conditioned and solutions may be unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_condition_number(x, d):\n",
    "    \"\"\"Compute condition number of X^T X\"\"\"\n",
    "    L_vec = [legendre(i) for i in range(d)]\n",
    "    X = np.column_stack([L_vec[i](x) for i in range(d)])\n",
    "    XTX = X.T @ X\n",
    "    return np.linalg.cond(XTX)\n",
    "\n",
    "# Analyze condition numbers for different scenarios\n",
    "print(\"Condition Numbers:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "scenarios = [\n",
    "    (x_N20, 5, \"d=5, N=20\"),\n",
    "    (x_N20, 20, \"d=20, N=20\"),\n",
    "    (x_N50, 20, \"d=20, N=50\"),\n",
    "    (x_N500, 20, \"d=20, N=500\"),\n",
    "    (x_N5000, 20, \"d=20, N=5000\"),\n",
    "]\n",
    "\n",
    "cond_numbers = []\n",
    "for x, d_val, label in scenarios:\n",
    "    cond = compute_condition_number(x, d_val)\n",
    "    cond_numbers.append(cond)\n",
    "    print(f\"{label}: κ(X^TX) = {cond:.2e}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Note: Condition number < 10⁶ is typically considered acceptable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training vs Test Error Curve\n",
    "\n",
    "This classic visualization shows how training and test errors diverge as model complexity increases.\n",
    "\n",
    "- **Underfitting** (low d): High bias, both training and test error are high\n",
    "- **Sweet spot** (optimal d): Low bias and variance, test error is minimized\n",
    "- **Overfitting** (high d): High variance, training error is low but test error is high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training vs test error curve\n",
    "N_train = 100\n",
    "x_train_curve, y_train_curve = generate_samples(gt_d5.f, N=N_train, mu=mu, std=std)\n",
    "x_test_curve, y_test_curve = generate_samples(gt_d5.f, N=1000, mu=mu, std=std)\n",
    "\n",
    "d_range_full = range(1, 40)\n",
    "train_errors_curve = []\n",
    "test_errors_curve = []\n",
    "\n",
    "for d_val in d_range_full:\n",
    "    model = LegendreRegression(d=d_val)\n",
    "    model.fit(x_train_curve, y_train_curve)\n",
    "    \n",
    "    train_pred = model.predict(x_train_curve)\n",
    "    test_pred = model.predict(x_test_curve)\n",
    "    \n",
    "    train_errors_curve.append(np.mean((y_train_curve - train_pred) ** 2))\n",
    "    test_errors_curve.append(np.mean((y_test_curve - test_pred) ** 2))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(d_range_full), train_errors_curve, 'b-o', label='Training Error', \n",
    "         linewidth=2, markersize=4)\n",
    "plt.plot(list(d_range_full), test_errors_curve, 'r-o', label='Test Error', \n",
    "         linewidth=2, markersize=4)\n",
    "plt.axvline(x=5, color='green', linestyle='--', alpha=0.7, label='True d=5')\n",
    "plt.axhline(y=std**2, color='gray', linestyle=':', label=f'$\\\\sigma^2$ = {std**2}')\n",
    "plt.xlabel('Model Complexity (d)', fontsize=12)\n",
    "plt.ylabel('MSE', fontsize=12)\n",
    "plt.title(f'Training vs Test Error (N={N_train})', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0, 0.5])\n",
    "plt.show()\n",
    "\n",
    "optimal_d_test = d_range_full[np.argmin(test_errors_curve)]\n",
    "print(f\"Optimal d (minimum test error): {optimal_d_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Coefficient Analysis\n",
    "\n",
    "Comparing learned coefficients with true coefficients helps us understand how well the model recovers the ground truth parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coefficients for d=5 models with different N\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Fit models with d=5 for different N\n",
    "coef_data = {}\n",
    "for N_val, x_data, y_data in [(20, x_N20, y_N20), (50, x_N50, y_N50), \n",
    "                               (500, x_N500, y_N500), (5000, x_N5000, y_N5000)]:\n",
    "    model = LegendreRegression(d=5)\n",
    "    model.fit(x_data, y_data)\n",
    "    coef_data[N_val] = model.theta\n",
    "\n",
    "# Plot\n",
    "x_pos = np.arange(5)\n",
    "width = 0.15\n",
    "multiplier = 0\n",
    "\n",
    "ax.bar(x_pos - 2*width, theta, width, label='True θ', color='black', alpha=0.8)\n",
    "for N_val, coefs in coef_data.items():\n",
    "    offset = width * multiplier\n",
    "    ax.bar(x_pos + offset - width, coefs, width, label=f'N={N_val}', alpha=0.7)\n",
    "    multiplier += 1\n",
    "\n",
    "ax.set_xlabel('Coefficient Index', fontsize=12)\n",
    "ax.set_ylabel('Coefficient Value', fontsize=12)\n",
    "ax.set_title('Learned Coefficients vs True Coefficients (d=5)', fontsize=14)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([f'θ₀', f'θ₁', f'θ₂', f'θ₃', f'θ₄'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print numerical comparison\n",
    "print(\"\\nCoefficient Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Coef':<6} {'True':<10} {'N=20':<10} {'N=50':<10} {'N=500':<10} {'N=5000':<10}\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(5):\n",
    "    print(f\"θ_{i:<4} {theta[i]:<10.4f} {coef_data[20][i]:<10.4f} {coef_data[50][i]:<10.4f} \"\n",
    "          f\"{coef_data[500][i]:<10.4f} {coef_data[5000][i]:<10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Learning Curve\n",
    "\n",
    "Learning curves show how training and test errors evolve as the training set size increases. This helps determine:\n",
    "- Whether collecting more data would help\n",
    "- If the model is limited by high bias (underfitting) or high variance (overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curve for d=5 and d=20\n",
    "def compute_learning_curve(d, N_values, n_experiments=30):\n",
    "    \"\"\"Compute average train and test errors for different N\"\"\"\n",
    "    train_means, train_stds = [], []\n",
    "    test_means, test_stds = [], []\n",
    "    \n",
    "    for N in N_values:\n",
    "        train_errs, test_errs = [], []\n",
    "        for _ in range(n_experiments):\n",
    "            x_train, y_train = generate_samples(gt_d5.f, N=N, mu=mu, std=std)\n",
    "            x_test_lc, y_test_lc = generate_samples(gt_d5.f, N=500, mu=mu, std=std)\n",
    "            \n",
    "            model = LegendreRegression(d=d)\n",
    "            model.fit(x_train, y_train)\n",
    "            \n",
    "            train_errs.append(np.mean((y_train - model.predict(x_train)) ** 2))\n",
    "            test_errs.append(np.mean((y_test_lc - model.predict(x_test_lc)) ** 2))\n",
    "        \n",
    "        train_means.append(np.mean(train_errs))\n",
    "        train_stds.append(np.std(train_errs))\n",
    "        test_means.append(np.mean(test_errs))\n",
    "        test_stds.append(np.std(test_errs))\n",
    "    \n",
    "    return train_means, train_stds, test_means, test_stds\n",
    "\n",
    "N_values = [10, 20, 30, 50, 75, 100, 150, 200, 300, 500]\n",
    "\n",
    "# Compute learning curves\n",
    "lc_d5 = compute_learning_curve(5, N_values, n_experiments=20)\n",
    "lc_d20 = compute_learning_curve(20, N_values, n_experiments=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# d=5 (correct model)\n",
    "axes[0].fill_between(N_values, \n",
    "                      np.array(lc_d5[0]) - np.array(lc_d5[1]), \n",
    "                      np.array(lc_d5[0]) + np.array(lc_d5[1]), alpha=0.2, color='blue')\n",
    "axes[0].fill_between(N_values, \n",
    "                      np.array(lc_d5[2]) - np.array(lc_d5[3]), \n",
    "                      np.array(lc_d5[2]) + np.array(lc_d5[3]), alpha=0.2, color='red')\n",
    "axes[0].plot(N_values, lc_d5[0], 'b-o', label='Train Error', linewidth=2, markersize=6)\n",
    "axes[0].plot(N_values, lc_d5[2], 'r-o', label='Test Error', linewidth=2, markersize=6)\n",
    "axes[0].axhline(y=std**2, color='gray', linestyle=':', label=f'$\\\\sigma^2$')\n",
    "axes[0].set_xlabel('Training Set Size (N)', fontsize=12)\n",
    "axes[0].set_ylabel('MSE', fontsize=12)\n",
    "axes[0].set_title('Learning Curve: d=5 (Correct Model)', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# d=20 (complex model)\n",
    "axes[1].fill_between(N_values, \n",
    "                      np.array(lc_d20[0]) - np.array(lc_d20[1]), \n",
    "                      np.array(lc_d20[0]) + np.array(lc_d20[1]), alpha=0.2, color='blue')\n",
    "axes[1].fill_between(N_values, \n",
    "                      np.array(lc_d20[2]) - np.array(lc_d20[3]), \n",
    "                      np.array(lc_d20[2]) + np.array(lc_d20[3]), alpha=0.2, color='red')\n",
    "axes[1].plot(N_values, lc_d20[0], 'b-o', label='Train Error', linewidth=2, markersize=6)\n",
    "axes[1].plot(N_values, lc_d20[2], 'r-o', label='Test Error', linewidth=2, markersize=6)\n",
    "axes[1].axhline(y=std**2, color='gray', linestyle=':', label=f'$\\\\sigma^2$')\n",
    "axes[1].set_xlabel('Training Set Size (N)', fontsize=12)\n",
    "axes[1].set_ylabel('MSE', fontsize=12)\n",
    "axes[1].set_title('Learning Curve: d=20 (Complex Model)', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook explored linear regression using Legendre polynomials, demonstrating:\n",
    "\n",
    "1. **Overfitting**: When $d \\geq N$, the model memorizes training data but fails to generalize.\n",
    "\n",
    "2. **Bias-Variance Tradeoff**: \n",
    "   - Low $d$: High bias, low variance (underfitting)\n",
    "   - High $d$: Low bias, high variance (overfitting)\n",
    "   - Optimal $d$: Balances both components\n",
    "\n",
    "3. **More Data Helps**: Increasing $N$ reduces overfitting even for complex models.\n",
    "\n",
    "4. **Training Error Formula**: $E_{\\text{train}} \\approx \\sigma^2(1 - d/N)$\n",
    "\n",
    "5. **Test Error Formula**: $E_{\\text{test}} \\approx \\sigma^2(1 + d/N)$\n",
    "\n",
    "6. **Regularization**: Ridge regression (L2 penalty) effectively controls overfitting.\n",
    "\n",
    "7. **Cross-Validation**: K-fold CV provides robust model selection without a separate test set.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The **ratio $d/N$** is critical: keep it small for good generalization\n",
    "- **Model selection** (choosing $d$) is as important as model fitting\n",
    "- **Regularization** provides an alternative to reducing model complexity\n",
    "- **Learning curves** help diagnose underfitting vs overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
